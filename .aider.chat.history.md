
# aider chat started at 2025-03-06 00:14:06

> You can skip this check with --no-gitignore  
> Add .aider*, .env to .gitignore (recommended)? (Y)es/(N)o [Yes]: y  
> Added .aider*, .env to .gitignore  
> /Users/dug/.local/bin/aider --model claude-3-7-sonnet-20250219 --yes-always --no-auto-commit --map-tokens 2048  
> Aider v0.75.2  
> Main model: claude-3-7-sonnet-20250219 with diff edit format, infinite output  
> Weak model: claude-3-5-haiku-20241022  
> Git repo: .git with 45 files  
> Repo-map: using 2048 tokens, auto refresh  

#### /add sql_gen.py  
> No files matched 'sql_gen.py'. Do you want to create /Users/dug/projects/Official Projects/dbt-tools/dbt_discovery_api/sql_gen.py? (Y)es/(N)o [Yes]: y  
> Added sql_gen.py to the chat  

#### /read-only sql_gen.py  
> Moved /Users/dug/projects/Official Projects/dbt-tools/dbt_discovery_api/sql_gen.py from editable to read-only files in the chat  

#### /add redshift_agent.py  
> No files matched 'redshift_agent.py'. Do you want to create /Users/dug/projects/Official Projects/dbt-tools/dbt_discovery_api/redshift_agent.py? (Y)es/(N)o [Yes]: y  
> Added redshift_agent.py to the chat  

#### CREATE redshift_agent.py MIRROR sql_gen.py - convert where needed to use Redshift functions. Ensure the agent knows its using redshift.  

I'll create a new `redshift_agent.py` file based on `sql_gen.py`, adapting it to use Redshift-specific functions and ensuring the agent knows it's working with Redshift.

redshift_agent.py
```python
<<<<<<< SEARCH
=======
import os
import re
import json
import logging
from typing import List, Dict, Any, Optional, Union, Tuple

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools.base import BaseTool
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RedshiftSQLGenerator:
    """
    A class that uses LLM to generate and refine Redshift SQL queries.
    """
    
    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.0):
        """
        Initialize the RedshiftSQLGenerator with a specific model.
        
        Args:
            model_name: The OpenAI model to use
            temperature: The temperature setting for the model
        """
        self.model_name = model_name
        self.temperature = temperature
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        
        # Create tools for the agent
        self.tools = self._create_tools()
        
        # Create the agent with the tools
        self.agent = self._create_agent()
        
        # Create the agent executor
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True,
        )
        
        # Initialize chat history
        self.chat_history = []
    
    def _create_tools(self) -> List[BaseTool]:
        """
        Create tools for the agent to use.
        
        Returns:
            A list of tools
        """
        # For now, we don't have any specific tools
        # This could be extended with database schema tools, etc.
        return []
    
    def _create_agent(self):
        """
        Create an agent with the tools.
        
        Returns:
            An agent
        """
        system_message = """You are an expert Redshift SQL developer. 
Your job is to write efficient, correct SQL queries for Redshift databases.

When writing SQL:
- Use Redshift-specific functions and syntax where appropriate
- Consider performance implications of your queries
- Use appropriate Redshift data types
- Follow Redshift best practices for query optimization
- Use DISTKEY and SORTKEY appropriately when creating tables
- Consider using Redshift's columnar storage optimizations
- Leverage Redshift's parallel processing capabilities
- Use appropriate compression encodings for columns

Redshift-specific functions to consider:
- COPY for data loading
- UNLOAD for data extraction
- VACUUM for table maintenance
- ANALYZE for statistics updates
- Window functions with OVER clause
- System tables like SVV_TABLE_INFO, STL_QUERY for monitoring
- Redshift Spectrum for querying data in S3

Always explain your reasoning and any Redshift-specific optimizations you've included.
"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        return create_openai_tools_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )
    
    def generate_sql(self, query_description: str) -> str:
        """
        Generate a SQL query based on a description.
        
        Args:
            query_description: A description of the query to generate
            
        Returns:
            A SQL query
        """
        # Add context about Redshift to the query description
        enhanced_description = f"""
Generate a Redshift SQL query that: {query_description}

Remember to use Redshift-specific features and optimizations where appropriate.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": enhanced_description,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=enhanced_description))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return sql
    
    def refine_sql(self, sql: str, refinement_instructions: str) -> str:
        """
        Refine an existing SQL query based on instructions.
        
        Args:
            sql: The SQL query to refine
            refinement_instructions: Instructions for how to refine the query
            
        Returns:
            A refined SQL query
        """
        # Add context about Redshift to the refinement instructions
        enhanced_instructions = f"""
Refine this Redshift SQL query:

```sql
{sql}
```

Refinement needed: {refinement_instructions}

Remember to use Redshift-specific features and optimizations where appropriate.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": enhanced_instructions,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        refined_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=enhanced_instructions))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return refined_sql
    
    def explain_sql(self, sql: str) -> str:
        """
        Explain a SQL query in natural language.
        
        Args:
            sql: The SQL query to explain
            
        Returns:
            A natural language explanation of the query
        """
        prompt = f"""
Explain this Redshift SQL query in detail:

```sql
{sql}
```

Include explanations of:
1. What the query does
2. Any Redshift-specific functions or optimizations
3. How the query is structured
4. Any performance considerations
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def optimize_sql(self, sql: str) -> str:
        """
        Optimize a SQL query for better performance.
        
        Args:
            sql: The SQL query to optimize
            
        Returns:
            An optimized SQL query
        """
        prompt = f"""
Optimize this Redshift SQL query for better performance:

```sql
{sql}
```

Consider Redshift-specific optimizations such as:
1. Appropriate DISTKEY and SORTKEY usage
2. Efficient join strategies
3. Avoiding expensive operations
4. Using appropriate compression encodings
5. Leveraging Redshift's parallel processing
6. Minimizing data movement between slices
7. Using appropriate Redshift functions

Explain your optimizations.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        optimized_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return optimized_sql
    
    def _extract_sql(self, text: str) -> str:
        """
        Extract SQL code from a text that may contain markdown code blocks.
        
        Args:
            text: The text to extract SQL from
            
        Returns:
            The extracted SQL
        """
        # Try to extract SQL from markdown code blocks
        sql_blocks = re.findall(r"```sql\n(.*?)\n```", text, re.DOTALL)
        
        if sql_blocks:
            return sql_blocks[0].strip()
        
        # If no SQL code blocks, try generic code blocks
        code_blocks = re.findall(r"```\n(.*?)\n```", text, re.DOTALL)
        
        if code_blocks:
            return code_blocks[0].strip()
        
        # If no code blocks at all, return the original text
        # This is a fallback and might not be actual SQL
        return text.strip()
    
    def clear_history(self):
        """
        Clear the chat history.
        """
        self.chat_history = []
    
    def generate_table_creation(self, table_description: str) -> str:
        """
        Generate a CREATE TABLE statement based on a description.
        
        Args:
            table_description: A description of the table to create
            
        Returns:
            A CREATE TABLE statement
        """
        prompt = f"""
Generate a Redshift CREATE TABLE statement based on this description:

{table_description}

Include appropriate:
1. Column data types optimized for Redshift
2. DISTKEY and SORTKEY definitions
3. Compression encodings where beneficial
4. Primary keys and foreign keys if needed
5. Any other Redshift-specific optimizations

Explain your choices for distribution style, sort keys, and compression.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        create_table_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return create_table_sql
    
    def generate_data_loading(self, table_name: str, data_source: str) -> str:
        """
        Generate a COPY command for loading data into Redshift.
        
        Args:
            table_name: The name of the table to load data into
            data_source: Description of the data source (S3 path, format, etc.)
            
        Returns:
            A COPY command
        """
        prompt = f"""
Generate a Redshift COPY command to load data into the table '{table_name}' from this source:

{data_source}

Include appropriate:
1. Data format specifications
2. Error handling options
3. IAM role or credentials method
4. Compression options if applicable
5. Any other relevant COPY command parameters

Explain your choices for the COPY command parameters.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        copy_command = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return copy_command
>>>>>>> REPLACE
```

I've created a new `redshift_agent.py` file that mirrors the structure of `sql_gen.py` but is specifically tailored for Redshift. The key changes include:

1. Renamed the main class to `RedshiftSQLGenerator`
2. Updated the system message to include Redshift-specific knowledge
3. Added Redshift-specific functions and optimizations in the prompts
4. Added methods for Redshift-specific operations like table creation with DISTKEY/SORTKEY and data loading with COPY commands
5. Included references to Redshift-specific features like compression encodings, distribution styles, and system tables

You can now use this agent to generate Redshift-optimized SQL queries.

> Tokens: 4.4k sent, 2.4k received. Cost: $0.05 message, $0.05 session.  
> Applied edit to redshift_agent.py  

#### remove all create functionality. Exclusively allow read. The main purpose of this agent is to work alongside the README.md dbt API to help identify redshift performane issues. This one specifically will need to know which redshift tables to look at  
> README.md  
> Add file to the chat? (Y)es/(N)o/(D)on't ask again [Yes]: y  

I'll modify the `redshift_agent.py` file to focus exclusively on read functionality for identifying Redshift performance issues, removing all create functionality, and ensuring it works alongside the dbt API described in the README.md.

redshift_agent.py
````python
<<<<<<< SEARCH
import os
import re
import json
import logging
from typing import List, Dict, Any, Optional, Union, Tuple

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools.base import BaseTool
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RedshiftSQLGenerator:
    """
    A class that uses LLM to generate and refine Redshift SQL queries.
    """
    
    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.0):
        """
        Initialize the RedshiftSQLGenerator with a specific model.
        
        Args:
            model_name: The OpenAI model to use
            temperature: The temperature setting for the model
        """
        self.model_name = model_name
        self.temperature = temperature
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        
        # Create tools for the agent
        self.tools = self._create_tools()
        
        # Create the agent with the tools
        self.agent = self._create_agent()
        
        # Create the agent executor
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True,
        )
        
        # Initialize chat history
        self.chat_history = []
    
    def _create_tools(self) -> List[BaseTool]:
        """
        Create tools for the agent to use.
        
        Returns:
            A list of tools
        """
        # For now, we don't have any specific tools
        # This could be extended with database schema tools, etc.
        return []
    
    def _create_agent(self):
        """
        Create an agent with the tools.
        
        Returns:
            An agent
        """
        system_message = """You are an expert Redshift SQL developer. 
Your job is to write efficient, correct SQL queries for Redshift databases.

When writing SQL:
- Use Redshift-specific functions and syntax where appropriate
- Consider performance implications of your queries
- Use appropriate Redshift data types
- Follow Redshift best practices for query optimization
- Use DISTKEY and SORTKEY appropriately when creating tables
- Consider using Redshift's columnar storage optimizations
- Leverage Redshift's parallel processing capabilities
- Use appropriate compression encodings for columns

Redshift-specific functions to consider:
- COPY for data loading
- UNLOAD for data extraction
- VACUUM for table maintenance
- ANALYZE for statistics updates
- Window functions with OVER clause
- System tables like SVV_TABLE_INFO, STL_QUERY for monitoring
- Redshift Spectrum for querying data in S3

Always explain your reasoning and any Redshift-specific optimizations you've included.
"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        return create_openai_tools_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )
    
    def generate_sql(self, query_description: str) -> str:
        """
        Generate a SQL query based on a description.
        
        Args:
            query_description: A description of the query to generate
            
        Returns:
            A SQL query
        """
        # Add context about Redshift to the query description
        enhanced_description = f"""
Generate a Redshift SQL query that: {query_description}

Remember to use Redshift-specific features and optimizations where appropriate.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": enhanced_description,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=enhanced_description))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return sql
    
    def refine_sql(self, sql: str, refinement_instructions: str) -> str:
        """
        Refine an existing SQL query based on instructions.
        
        Args:
            sql: The SQL query to refine
            refinement_instructions: Instructions for how to refine the query
            
        Returns:
            A refined SQL query
        """
        # Add context about Redshift to the refinement instructions
        enhanced_instructions = f"""
Refine this Redshift SQL query:

```sql
{sql}
```

Refinement needed: {refinement_instructions}

Remember to use Redshift-specific features and optimizations where appropriate.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": enhanced_instructions,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        refined_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=enhanced_instructions))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return refined_sql
    
    def explain_sql(self, sql: str) -> str:
        """
        Explain a SQL query in natural language.
        
        Args:
            sql: The SQL query to explain
            
        Returns:
            A natural language explanation of the query
        """
        prompt = f"""
Explain this Redshift SQL query in detail:

```sql
{sql}
```

Include explanations of:
1. What the query does
2. Any Redshift-specific functions or optimizations
3. How the query is structured
4. Any performance considerations
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def optimize_sql(self, sql: str) -> str:
        """
        Optimize a SQL query for better performance.
        
        Args:
            sql: The SQL query to optimize
            
        Returns:
            An optimized SQL query
        """
        prompt = f"""
Optimize this Redshift SQL query for better performance:

```sql
{sql}
```

Consider Redshift-specific optimizations such as:
1. Appropriate DISTKEY and SORTKEY usage
2. Efficient join strategies
3. Avoiding expensive operations
4. Using appropriate compression encodings
5. Leveraging Redshift's parallel processing
6. Minimizing data movement between slices
7. Using appropriate Redshift functions

Explain your optimizations.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        optimized_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return optimized_sql
    
    def _extract_sql(self, text: str) -> str:
        """
        Extract SQL code from a text that may contain markdown code blocks.
        
        Args:
            text: The text to extract SQL from
            
        Returns:
            The extracted SQL
        """
        # Try to extract SQL from markdown code blocks
        sql_blocks = re.findall(r"```sql\n(.*?)\n```", text, re.DOTALL)
        
        if sql_blocks:
            return sql_blocks[0].strip()
        
        # If no SQL code blocks, try generic code blocks
        code_blocks = re.findall(r"```\n(.*?)\n```", text, re.DOTALL)
        
        if code_blocks:
            return code_blocks[0].strip()
        
        # If no code blocks at all, return the original text
        # This is a fallback and might not be actual SQL
        return text.strip()
    
    def clear_history(self):
        """
        Clear the chat history.
        """
        self.chat_history = []
    
    def generate_table_creation(self, table_description: str) -> str:
        """
        Generate a CREATE TABLE statement based on a description.
        
        Args:
            table_description: A description of the table to create
            
        Returns:
            A CREATE TABLE statement
        """
        prompt = f"""
Generate a Redshift CREATE TABLE statement based on this description:

{table_description}

Include appropriate:
1. Column data types optimized for Redshift
2. DISTKEY and SORTKEY definitions
3. Compression encodings where beneficial
4. Primary keys and foreign keys if needed
5. Any other Redshift-specific optimizations

Explain your choices for distribution style, sort keys, and compression.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        create_table_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return create_table_sql
    
    def generate_data_loading(self, table_name: str, data_source: str) -> str:
        """
        Generate a COPY command for loading data into Redshift.
        
        Args:
            table_name: The name of the table to load data into
            data_source: Description of the data source (S3 path, format, etc.)
            
        Returns:
            A COPY command
        """
        prompt = f"""
Generate a Redshift COPY command to load data into the table '{table_name}' from this source:

{data_source}

Include appropriate:
1. Data format specifications
2. Error handling options
3. IAM role or credentials method
4. Compression options if applicable
5. Any other relevant COPY command parameters

Explain your choices for the COPY command parameters.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        copy_command = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return copy_command
=======
import os
import re
import json
import logging
from typing import List, Dict, Any, Optional, Union, Tuple

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools.base import BaseTool
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RedshiftPerformanceAnalyzer:
    """
    A class that uses LLM to analyze and diagnose Redshift performance issues.
    Works alongside the dbt Discovery API to identify performance bottlenecks.
    """
    
    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.0):
        """
        Initialize the RedshiftPerformanceAnalyzer with a specific model.
        
        Args:
            model_name: The OpenAI model to use
            temperature: The temperature setting for the model
        """
        self.model_name = model_name
        self.temperature = temperature
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        
        # Create tools for the agent
        self.tools = self._create_tools()
        
        # Create the agent with the tools
        self.agent = self._create_agent()
        
        # Create the agent executor
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True,
        )
        
        # Initialize chat history
        self.chat_history = []
    
    def _create_tools(self) -> List[BaseTool]:
        """
        Create tools for the agent to use.
        
        Returns:
            A list of tools
        """
        # For now, we don't have any specific tools
        # This could be extended with database schema tools, etc.
        return []
    
    def _create_agent(self):
        """
        Create an agent with the tools.
        
        Returns:
            An agent
        """
        system_message = """You are an expert Redshift performance analyst. 
Your job is to analyze and diagnose performance issues in Redshift databases.

When analyzing Redshift performance:
- Focus on identifying bottlenecks in query execution
- Recommend appropriate system tables to query for diagnostics
- Interpret query plans and execution statistics
- Identify distribution style and sort key issues
- Detect data skew problems
- Recognize inefficient join patterns
- Identify missing or outdated statistics

Key Redshift system tables to query:
- SVV_TABLE_INFO: Table metadata including distribution style, sort keys
- STL_QUERY: Query execution history
- STL_QUERY_METRICS: Detailed metrics about query steps
- STL_ALERT_EVENT_LOG: System-generated alerts
- SVL_QUERY_SUMMARY: Summary information for queries
- SVL_QUERY_REPORT: Step-level execution details
- SVV_QUERY_INFLIGHT: Currently running queries
- STV_INFLIGHT: Information about currently running queries
- STV_BLOCKLIST: Data block distribution
- STV_NODE_STORAGE_CAPACITY: Storage capacity and utilization

Always explain your reasoning and provide specific SQL queries to diagnose issues.
"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        return create_openai_tools_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )
    
    def generate_diagnostic_query(self, issue_description: str) -> str:
        """
        Generate a diagnostic SQL query based on a performance issue description.
        
        Args:
            issue_description: A description of the performance issue to diagnose
            
        Returns:
            A diagnostic SQL query
        """
        enhanced_description = f"""
Generate a Redshift diagnostic SQL query to investigate this performance issue: {issue_description}

Focus on using appropriate Redshift system tables and views to diagnose the problem.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": enhanced_description,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=enhanced_description))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return sql
    
    def analyze_query_performance(self, query_id: str) -> str:
        """
        Analyze the performance of a specific query by its ID.
        
        Args:
            query_id: The ID of the query to analyze
            
        Returns:
            Analysis of the query performance
        """
        prompt = f"""
Analyze the performance of Redshift query with ID {query_id}.

Generate SQL queries that use Redshift system tables to:
1. Get detailed execution metrics for this query
2. Identify bottlenecks in the query execution
3. Suggest potential optimizations

Include specific system tables like STL_QUERY, STL_QUERY_METRICS, SVL_QUERY_SUMMARY, etc.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def analyze_table_structure(self, table_name: str) -> str:
        """
        Analyze the structure of a specific table for performance issues.
        
        Args:
            table_name: The name of the table to analyze
            
        Returns:
            Analysis of the table structure
        """
        prompt = f"""
Analyze the structure of Redshift table '{table_name}' for potential performance issues.

Generate SQL queries that use Redshift system tables to:
1. Check the distribution style and sort keys
2. Analyze data skew
3. Check compression encodings
4. Verify statistics are up to date
5. Identify potential distribution or sort key improvements

Include specific system tables like SVV_TABLE_INFO, STV_BLOCKLIST, etc.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def identify_slow_queries(self, time_period: str = "1 day") -> str:
        """
        Identify slow queries within a specific time period.
        
        Args:
            time_period: The time period to analyze (e.g., "1 day", "1 week")
            
        Returns:
            SQL query to identify slow queries
        """
        prompt = f"""
Generate a Redshift SQL query to identify slow queries from the past {time_period}.

The query should:
1. Use appropriate system tables like STL_QUERY, SVL_QUERY_SUMMARY
2. Sort by execution time or other relevant performance metrics
3. Include query text, execution time, and other useful diagnostic information
4. Filter out system queries and focus on user queries
5. Provide context about what might be causing slowness
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return sql
    
    def analyze_workload_patterns(self) -> str:
        """
        Analyze workload patterns to identify peak usage and resource contention.
        
        Returns:
            Analysis of workload patterns
        """
        prompt = """
Generate Redshift SQL queries to analyze workload patterns and identify resource contention.

The queries should:
1. Identify peak usage times
2. Detect query queuing and WLM queue wait times
3. Analyze concurrency issues
4. Identify long-running queries that might block others
5. Check for appropriate WLM configuration

Use system tables like STL_QUERY, STL_WLM_QUERY, STL_COMMIT_STATS, etc.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def analyze_dbt_model_performance(self, model_name: str) -> str:
        """
        Analyze the performance of a specific dbt model in Redshift.
        
        Args:
            model_name: The name of the dbt model to analyze
            
        Returns:
            Analysis of the model's performance in Redshift
        """
        prompt = f"""
Generate Redshift SQL queries to analyze the performance of dbt model '{model_name}'.

Assume this model is materialized as a table or view in Redshift.

The queries should:
1. Check the table structure and distribution style
2. Analyze query patterns that access this model
3. Identify potential bottlenecks when this model is used in joins
4. Check for appropriate sort keys and distribution keys
5. Analyze data skew if present

Use system tables like SVV_TABLE_INFO, STL_QUERY, SVL_QUERY_SUMMARY, etc.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def _extract_sql(self, text: str) -> str:
        """
        Extract SQL code from a text that may contain markdown code blocks.
        
        Args:
            text: The text to extract SQL from
            
        Returns:
            The extracted SQL
        """
        # Try to extract SQL from markdown code blocks
        sql_blocks = re.findall(r"```sql\n(.*?)\n```", text, re.DOTALL)
        
        if sql_blocks:
            return sql_blocks[0].strip()
        
        # If no SQL code blocks, try generic code blocks
        code_blocks = re.findall(r"```\n(.*?)\n```", text, re.DOTALL)
        
        if code_blocks:
            return code_blocks[0].strip()
        
        # If no code blocks at all, return the original text
        # This is a fallback and might not be actual SQL
        return text.strip()
    
    def clear_history(self):
        """
        Clear the chat history.
        """
        self.chat_history = []
    
    def explain_system_table(self, table_name: str) -> str:
        """
        Explain a Redshift system table and how to use it for performance analysis.
        
        Args:
            table_name: The name of the system table to explain
            
        Returns:
            Explanation of the system table
        """
        prompt = f"""
Explain the Redshift system table '{table_name}' in detail:

1. What information does this table contain?
2. How is it useful for performance analysis?
3. What are the key columns to focus on?
4. Provide example queries that use this table for performance diagnostics
5. Mention any limitations or considerations when using this table
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def recommend_monitoring_queries(self) -> str:
        """
        Recommend a set of monitoring queries for ongoing Redshift performance tracking.
        
        Returns:
            A set of recommended monitoring queries
        """
        prompt = """
Recommend a set of Redshift SQL queries for ongoing performance monitoring.

Include queries that:
1. Track long-running queries
2. Monitor table usage and access patterns
3. Identify data skew issues
4. Track query queue wait times
5. Monitor disk usage and storage
6. Identify tables that need VACUUM or ANALYZE
7. Track query performance trends over time

Explain why each query is useful and how to interpret the results.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
>>>>>>> REPLACE
````

I've completely refactored the file to:

1. Rename the class from `RedshiftSQLGenerator` to `RedshiftPerformanceAnalyzer` to better reflect its purpose
2. Removed all create functionality (table creation, data loading)
3. Updated the system message to focus on performance analysis rather than SQL generation
4. Added methods specifically for performance diagnostics:
   - `generate_diagnostic_query`: Creates queries to diagnose specific issues
   - `analyze_query_performance`: Analyzes performance of a specific query
   - `analyze_table_structure`: Examines table structure for performance issues
   - `identify_slow_queries`: Finds slow-running queries
   - `analyze_workload_patterns`: Examines resource usage patterns
   - `analyze_dbt_model_performance`: Specifically analyzes dbt models
   - `explain_system_table`: Explains Redshift system tables
   - `recommend_monitoring_queries`: Suggests monitoring queries

The agent now focuses exclusively on reading from Redshift system tables to diagnose performance issues, with specific knowledge about which tables to query for different types of problems. It's designed to work alongside the dbt Discovery API described in the README.md.

> Tokens: 11k sent, 5.3k received. Cost: $0.11 message, $0.16 session.  
> Applied edit to redshift_agent.py  

# aider chat started at 2025-03-12 15:45:21

> Newer aider version v0.76.2 is available.  
> /Users/doug/.local/share/uv/tools/aider-chat/bin/python3 -m pip install --upgrade --upgrade-strategy only-if-needed aider-chat  
> Run pip install? (Y)es/(N)o [Yes]: y  
> Re-run aider to use new version.  

# aider chat started at 2025-03-12 15:45:36

> You can skip this check with --no-gitignore  
> Add .aider*, .env to .gitignore (recommended)? (Y)es/(N)o [Yes]: y  
> Added .aider*, .env to .gitignore  
> /Users/doug/.local/bin/aider --model anthropic/claude-3-7-sonnet-20250219 --no-auto-commit --yes-always  
> Aider v0.76.2  
> Main model: anthropic/claude-3-7-sonnet-20250219 with diff edit format, infinite output  
> Weak model: anthropic/claude-3-5-haiku-20241022  
> Git repo: .git with 55 files  
> Repo-map: using 4096 tokens, auto refresh  
> https://aider.chat/HISTORY.html#release-notes  
> Would you like to see what's new in this version? (Y)es/(N)o [Yes]: y  
>  
>  
> ^C again to exit  
>  
>  
> ^C KeyboardInterrupt  

# aider chat started at 2025-03-12 15:45:57

> You can skip this check with --no-gitignore  
> Add .env to .gitignore (recommended)? (Y)es/(N)o [Yes]: y  
> Added .env to .gitignore  
> /Users/doug/.local/bin/aider --model anthropic/claude-3-7-sonnet-20250219 --no-auto-commit --yes-always  
> Aider v0.76.2  
> Main model: anthropic/claude-3-7-sonnet-20250219 with diff edit format, infinite output  
> Weak model: anthropic/claude-3-5-haiku-20241022  
> Git repo: .git with 57 files  
> Repo-map: using 4096 tokens, auto refresh  

#### /add src/discovery_api/services/JobService.py  
> Added src/discovery_api/services/JobService.py to the chat  

#### UPDATE JobService:  
####     UPDATE _add_model_fields(), add_test_fields():  
####         add kwargs** support:  
####             for non Basic information fields, create logic block to optionally add various fields  

I'll update the JobService class to add kwargs support for the model and test field methods, allowing optional inclusion of different field groups. Here are the changes:

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def _add_model_fields(self, models: Operation):
        """Add model fields to the query."""
        # Basic model information
        models.name()
        models.unique_id()
        models.description()
        models.tags()
        models.resource_type()
        
        # Database information
        models.database()
        models.schema()
        models.alias()
        
        # Run metadata
        models.run_id()
        models.job_id()
        models.invocation_id()
        models.thread_id()
        
        # Execution timing
        models.run_generated_at()
        models.compile_started_at()
        models.compile_completed_at()
        models.execute_started_at()
        models.execute_completed_at()
        models.execution_time()
        models.run_elapsed_time()
        
        # Status information
        models.status()
        models.error()
        models.skip()
        
        # Code fields
        models.raw_sql()
        models.compiled_sql()
        models.raw_code()
        models.compiled_code()
=======
    def _add_model_fields(self, models: Operation, **kwargs):
        """
        Add model fields to the query.
        
        Args:
            models: The Operation object to add fields to
            **kwargs: Optional field groups to include:
                include_database (bool): Include database fields
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
        """
        # Default field groups to include
        include_all = kwargs.get('include_all', False)
        include_database = kwargs.get('include_database', include_all)
        include_run_metadata = kwargs.get('include_run_metadata', include_all)
        include_timing = kwargs.get('include_timing', include_all)
        include_status = kwargs.get('include_status', include_all)
        include_code = kwargs.get('include_code', include_all)
        
        # Basic model information (always included)
        models.name()
        models.unique_id()
        models.description()
        models.tags()
        models.resource_type()
        
        # Database information
        if include_database:
            models.database()
            models.schema()
            models.alias()
        
        # Run metadata
        if include_run_metadata:
            models.run_id()
            models.job_id()
            models.invocation_id()
            models.thread_id()
        
        # Execution timing
        if include_timing:
            models.run_generated_at()
            models.compile_started_at()
            models.compile_completed_at()
            models.execute_started_at()
            models.execute_completed_at()
            models.execution_time()
            models.run_elapsed_time()
        
        # Status information
        if include_status:
            models.status()
            models.error()
            models.skip()
        
        # Code fields
        if include_code:
            models.raw_sql()
            models.compiled_sql()
            models.raw_code()
            models.compiled_code()
>>>>>>> REPLACE
```

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def _add_test_fields(self, tests: Operation):
        """Add test fields to the query."""
        # Basic test information
        tests.name()
        tests.unique_id()
        tests.description()
        tests.column_name()
        tests.tags()
        tests.resource_type()
        
        # Run metadata
        tests.run_id()
        tests.job_id()
        tests.invocation_id()
        
        # Execution timing
        tests.run_generated_at()
        tests.compile_started_at()
        tests.compile_completed_at()
        tests.execute_started_at()
        tests.execute_completed_at()
        tests.execution_time()
        tests.run_elapsed_time()
        
        # Status information
        tests.status()
        tests.error()
        tests.state()
        tests.warn()
        tests.fail()
        tests.skip()
        
        # Code fields
        tests.raw_sql()
        tests.compiled_sql()
        tests.raw_code()
        tests.compiled_code()
=======
    def _add_test_fields(self, tests: Operation, **kwargs):
        """
        Add test fields to the query.
        
        Args:
            tests: The Operation object to add fields to
            **kwargs: Optional field groups to include:
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
        """
        # Default field groups to include
        include_all = kwargs.get('include_all', False)
        include_run_metadata = kwargs.get('include_run_metadata', include_all)
        include_timing = kwargs.get('include_timing', include_all)
        include_status = kwargs.get('include_status', include_all)
        include_code = kwargs.get('include_code', include_all)
        
        # Basic test information (always included)
        tests.name()
        tests.unique_id()
        tests.description()
        tests.column_name()
        tests.tags()
        tests.resource_type()
        
        # Run metadata
        if include_run_metadata:
            tests.run_id()
            tests.job_id()
            tests.invocation_id()
        
        # Execution timing
        if include_timing:
            tests.run_generated_at()
            tests.compile_started_at()
            tests.compile_completed_at()
            tests.execute_started_at()
            tests.execute_completed_at()
            tests.execution_time()
            tests.run_elapsed_time()
        
        # Status information
        if include_status:
            tests.status()
            tests.error()
            tests.state()
            tests.warn()
            tests.fail()
            tests.skip()
        
        # Code fields
        if include_code:
            tests.raw_sql()
            tests.compiled_sql()
            tests.raw_code()
            tests.compiled_code()
>>>>>>> REPLACE
```

Now we need to update the methods that call these functions to pass the kwargs:

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def get_job_models(self, job_id: int, **kwargs) -> Dict[str, Any]:
        """
        Get models for a specific job.
        
        Args:
            job_id: The ID of the job to query
            **kwargs: Additional arguments to pass to execute, such as return_query=True
            
        Returns:
            Dictionary containing job models data
        """
        op, job = self.base_query.create_job_query(job_id)
        models = job.models()
        self._add_model_fields(models)
=======
    def get_job_models(self, job_id: int, **kwargs) -> Dict[str, Any]:
        """
        Get models for a specific job.
        
        Args:
            job_id: The ID of the job to query
            **kwargs: Additional arguments to pass to execute or field selection:
                include_database (bool): Include database fields
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
                return_query (bool): Return the query instead of executing it
            
        Returns:
            Dictionary containing job models data
        """
        # Extract execution kwargs from field selection kwargs
        exec_kwargs = {k: v for k, v in kwargs.items() if k in ['return_query']}
        field_kwargs = {k: v for k, v in kwargs.items() if k not in exec_kwargs}
        
        op, job = self.base_query.create_job_query(job_id)
        models = job.models()
        self._add_model_fields(models, **field_kwargs)
>>>>>>> REPLACE
```

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def get_job_tests(self, job_id: int, **kwargs) -> Dict[str, Any]:
        """
        Get tests for a specific job.
        
        Args:
            job_id: The ID of the job to query
            **kwargs: Additional arguments to pass to execute, such as return_query=True
            
        Returns:
            Dictionary containing job tests data
        """
        op, job = self.base_query.create_job_query(job_id)
        tests = job.tests()
        self._add_test_fields(tests)
        
        response = self.base_query.execute(op, **kwargs)
=======
    def get_job_tests(self, job_id: int, **kwargs) -> Dict[str, Any]:
        """
        Get tests for a specific job.
        
        Args:
            job_id: The ID of the job to query
            **kwargs: Additional arguments to pass to execute or field selection:
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
                return_query (bool): Return the query instead of executing it
            
        Returns:
            Dictionary containing job tests data
        """
        # Extract execution kwargs from field selection kwargs
        exec_kwargs = {k: v for k, v in kwargs.items() if k in ['return_query']}
        field_kwargs = {k: v for k, v in kwargs.items() if k not in exec_kwargs}
        
        op, job = self.base_query.create_job_query(job_id)
        tests = job.tests()
        self._add_test_fields(tests, **field_kwargs)
        
        response = self.base_query.execute(op, **exec_kwargs)
>>>>>>> REPLACE
```

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def get_job_model_by_unique_id(self, job_id: int, unique_id: str, **kwargs) -> Dict[str, Any]:
        """
        Get a specific model by unique_id from a job.
        
        Args:
            job_id: The ID of the job to query
            unique_id: The unique ID of the model to retrieve
            **kwargs: Additional arguments to pass to execute, such as return_query=True
            
        Returns:
            Dictionary containing the model data
        """
        op, job = self.base_query.create_job_query(job_id)
        model = job.model(unique_id=unique_id)
        self._add_model_fields(model)
        
        response = self.base_query.execute(op, **kwargs)
=======
    def get_job_model_by_unique_id(self, job_id: int, unique_id: str, **kwargs) -> Dict[str, Any]:
        """
        Get a specific model by unique_id from a job.
        
        Args:
            job_id: The ID of the job to query
            unique_id: The unique ID of the model to retrieve
            **kwargs: Additional arguments to pass to execute or field selection:
                include_database (bool): Include database fields
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
                return_query (bool): Return the query instead of executing it
            
        Returns:
            Dictionary containing the model data
        """
        # Extract execution kwargs from field selection kwargs
        exec_kwargs = {k: v for k, v in kwargs.items() if k in ['return_query']}
        field_kwargs = {k: v for k, v in kwargs.items() if k not in exec_kwargs}
        
        op, job = self.base_query.create_job_query(job_id)
        model = job.model(unique_id=unique_id)
        self._add_model_fields(model, **field_kwargs)
        
        response = self.base_query.execute(op, **exec_kwargs)
>>>>>>> REPLACE
```

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def get_job_test_by_unique_id(self, job_id: int, unique_id: str, **kwargs) -> Dict[str, Any]:
        """
        Get a specific test by unique_id from a job.
        
        Args:
            job_id: The ID of the job to query
            unique_id: The unique ID of the test to retrieve
            **kwargs: Additional arguments to pass to execute, such as return_query=True
            
        Returns:
            Dictionary containing the test data
        """
        op, job = self.base_query.create_job_query(job_id)
        test = job.test(unique_id=unique_id)
        self._add_test_fields(test)
        
        response = self.base_query.execute(op, **kwargs)
=======
    def get_job_test_by_unique_id(self, job_id: int, unique_id: str, **kwargs) -> Dict[str, Any]:
        """
        Get a specific test by unique_id from a job.
        
        Args:
            job_id: The ID of the job to query
            unique_id: The unique ID of the test to retrieve
            **kwargs: Additional arguments to pass to execute or field selection:
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
                return_query (bool): Return the query instead of executing it
            
        Returns:
            Dictionary containing the test data
        """
        # Extract execution kwargs from field selection kwargs
        exec_kwargs = {k: v for k, v in kwargs.items() if k in ['return_query']}
        field_kwargs = {k: v for k, v in kwargs.items() if k not in exec_kwargs}
        
        op, job = self.base_query.create_job_query(job_id)
        test = job.test(unique_id=unique_id)
        self._add_test_fields(test, **field_kwargs)
        
        response = self.base_query.execute(op, **exec_kwargs)
>>>>>>> REPLACE
```

Finally, update the get_job_with_models_and_tests method:

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def get_job_with_models_and_tests(self, job_id: int, **kwargs) -> Dict[str, Any]:
        """
        Get job data with models and tests.
        
        Args:
            job_id: The ID of the job to query
            **kwargs: Additional arguments to pass to execute, such as return_query=True
            
        Returns:
            Dictionary containing job data with models and tests
        """
        op, job = self.base_query.create_job_query(job_id)
        
        # Add job fields
        self._add_job_fields(job)
        
        # Add models data
        models = job.models()
        self._add_model_fields(models)
        
        # Add tests data
        tests = job.tests()
        self._add_test_fields(tests)
        
        response = self.base_query.execute(op, **kwargs)
=======
    def get_job_with_models_and_tests(self, job_id: int, **kwargs) -> Dict[str, Any]:
        """
        Get job data with models and tests.
        
        Args:
            job_id: The ID of the job to query
            **kwargs: Additional arguments to pass to execute or field selection:
                include_database (bool): Include database fields for models
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
                return_query (bool): Return the query instead of executing it
            
        Returns:
            Dictionary containing job data with models and tests
        """
        # Extract execution kwargs from field selection kwargs
        exec_kwargs = {k: v for k, v in kwargs.items() if k in ['return_query']}
        field_kwargs = {k: v for k, v in kwargs.items() if k not in exec_kwargs}
        
        op, job = self.base_query.create_job_query(job_id)
        
        # Add job fields
        self._add_job_fields(job)
        
        # Add models data
        models = job.models()
        self._add_model_fields(models, **field_kwargs)
        
        # Add tests data
        tests = job.tests()
        self._add_test_fields(tests, **field_kwargs)
        
        response = self.base_query.execute(op, **exec_kwargs)
>>>>>>> REPLACE
```

> Tokens: 7.8k sent, 3.4k received. Cost: $0.07 message, $0.07 session.  
> Applied edit to src/discovery_api/services/JobService.py  

#### /add tests/test_job_service.py  
> Added tests/test_job_service.py to the chat  

#### UPDATE tests accordingly to test new functionality.  

I'll update the test file to test the new functionality with kwargs support for field selection. Here are the changes:

tests/test_job_service.py
```python
<<<<<<< SEARCH
    def test_add_model_fields(self):
        """Test _add_model_fields method."""
        mock_model = MagicMock()
        self.job_service._add_model_fields(mock_model)
        
        # Verify basic fields are added
        mock_model.name.assert_called_once()
        mock_model.unique_id.assert_called_once()
        mock_model.description.assert_called_once()
        mock_model.tags.assert_called_once()
        mock_model.resource_type.assert_called_once()
        
        # Verify database fields are added
        mock_model.database.assert_called_once()
        mock_model.schema.assert_called_once()
        mock_model.alias.assert_called_once()
        
        # Verify run metadata fields are added
        mock_model.run_id.assert_called_once()
        mock_model.job_id.assert_called_once()
        
        # Verify execution timing fields are added
        mock_model.execution_time.assert_called_once()
        mock_model.run_elapsed_time.assert_called_once()
        
        # Verify status fields are added
        mock_model.status.assert_called_once()
        mock_model.error.assert_called_once()
        
        # Verify code fields are added
        mock_model.raw_sql.assert_called_once()
        mock_model.compiled_sql.assert_called_once()
=======
    def test_add_model_fields(self):
        """Test _add_model_fields method with default settings (all fields)."""
        mock_model = MagicMock()
        self.job_service._add_model_fields(mock_model)
        
        # Verify basic fields are added
        mock_model.name.assert_called_once()
        mock_model.unique_id.assert_called_once()
        mock_model.description.assert_called_once()
        mock_model.tags.assert_called_once()
        mock_model.resource_type.assert_called_once()
        
        # Verify database fields are added
        mock_model.database.assert_called_once()
        mock_model.schema.assert_called_once()
        mock_model.alias.assert_called_once()
        
        # Verify run metadata fields are added
        mock_model.run_id.assert_called_once()
        mock_model.job_id.assert_called_once()
        
        # Verify execution timing fields are added
        mock_model.execution_time.assert_called_once()
        mock_model.run_elapsed_time.assert_called_once()
        
        # Verify status fields are added
        mock_model.status.assert_called_once()
        mock_model.error.assert_called_once()
        
        # Verify code fields are added
        mock_model.raw_sql.assert_called_once()
        mock_model.compiled_sql.assert_called_once()
    
    def test_add_model_fields_with_selective_fields(self):
        """Test _add_model_fields method with selective field groups."""
        # Test with only database fields
        mock_model = MagicMock()
        self.job_service._add_model_fields(mock_model, include_database=True)
        
        # Basic fields should always be included
        mock_model.name.assert_called_once()
        mock_model.unique_id.assert_called_once()
        mock_model.resource_type.assert_called_once()
        
        # Database fields should be included
        mock_model.database.assert_called_once()
        mock_model.schema.assert_called_once()
        
        # Run metadata fields should not be included
        mock_model.run_id.assert_not_called()
        mock_model.job_id.assert_not_called()
        
        # Code fields should not be included
        mock_model.raw_sql.assert_not_called()
        mock_model.compiled_sql.assert_not_called()
        
        # Test with only code fields
        mock_model = MagicMock()
        self.job_service._add_model_fields(mock_model, include_code=True)
        
        # Basic fields should always be included
        mock_model.name.assert_called_once()
        mock_model.unique_id.assert_called_once()
        
        # Database fields should not be included
        mock_model.database.assert_not_called()
        mock_model.schema.assert_not_called()
        
        # Code fields should be included
        mock_model.raw_sql.assert_called_once()
        mock_model.compiled_sql.assert_called_once()
        
        # Test with include_all=True
        mock_model = MagicMock()
        self.job_service._add_model_fields(mock_model, include_all=True)
        
        # All field groups should be included
        mock_model.name.assert_called_once()
        mock_model.database.assert_called_once()
        mock_model.run_id.assert_called_once()
        mock_model.execution_time.assert_called_once()
        mock_model.status.assert_called_once()
        mock_model.raw_sql.assert_called_once()
>>>>>>> REPLACE
```

tests/test_job_service.py
```python
<<<<<<< SEARCH
    def test_add_test_fields(self):
        """Test _add_test_fields method."""
        mock_test = MagicMock()
        self.job_service._add_test_fields(mock_test)
        
        # Verify basic fields are added
        mock_test.name.assert_called_once()
        mock_test.unique_id.assert_called_once()
        mock_test.description.assert_called_once()
        mock_test.tags.assert_called_once()
        mock_test.resource_type.assert_called_once()
        
        # Verify run metadata fields are added
        mock_test.run_id.assert_called_once()
        mock_test.job_id.assert_called_once()
        
        # Verify execution timing fields are added
        mock_test.execution_time.assert_called_once()
        mock_test.run_elapsed_time.assert_called_once()
        
        # Verify status fields are added
        mock_test.status.assert_called_once()
        mock_test.error.assert_called_once()
        mock_test.state.assert_called_once()
        mock_test.fail.assert_called_once()
        
        # Verify code fields are added
        mock_test.raw_sql.assert_called_once()
        mock_test.compiled_sql.assert_called_once()
=======
    def test_add_test_fields(self):
        """Test _add_test_fields method with default settings (all fields)."""
        mock_test = MagicMock()
        self.job_service._add_test_fields(mock_test)
        
        # Verify basic fields are added
        mock_test.name.assert_called_once()
        mock_test.unique_id.assert_called_once()
        mock_test.description.assert_called_once()
        mock_test.tags.assert_called_once()
        mock_test.resource_type.assert_called_once()
        
        # Verify run metadata fields are added
        mock_test.run_id.assert_called_once()
        mock_test.job_id.assert_called_once()
        
        # Verify execution timing fields are added
        mock_test.execution_time.assert_called_once()
        mock_test.run_elapsed_time.assert_called_once()
        
        # Verify status fields are added
        mock_test.status.assert_called_once()
        mock_test.error.assert_called_once()
        mock_test.state.assert_called_once()
        mock_test.fail.assert_called_once()
        
        # Verify code fields are added
        mock_test.raw_sql.assert_called_once()
        mock_test.compiled_sql.assert_called_once()
    
    def test_add_test_fields_with_selective_fields(self):
        """Test _add_test_fields method with selective field groups."""
        # Test with only status fields
        mock_test = MagicMock()
        self.job_service._add_test_fields(mock_test, include_status=True)
        
        # Basic fields should always be included
        mock_test.name.assert_called_once()
        mock_test.unique_id.assert_called_once()
        mock_test.resource_type.assert_called_once()
        
        # Status fields should be included
        mock_test.status.assert_called_once()
        mock_test.error.assert_called_once()
        mock_test.fail.assert_called_once()
        
        # Run metadata fields should not be included
        mock_test.run_id.assert_not_called()
        mock_test.job_id.assert_not_called()
        
        # Code fields should not be included
        mock_test.raw_sql.assert_not_called()
        mock_test.compiled_sql.assert_not_called()
        
        # Test with only timing fields
        mock_test = MagicMock()
        self.job_service._add_test_fields(mock_test, include_timing=True)
        
        # Basic fields should always be included
        mock_test.name.assert_called_once()
        mock_test.unique_id.assert_called_once()
        
        # Timing fields should be included
        mock_test.execution_time.assert_called_once()
        mock_test.run_elapsed_time.assert_called_once()
        
        # Status fields should not be included
        mock_test.status.assert_not_called()
        mock_test.error.assert_not_called()
        
        # Test with include_all=True
        mock_test = MagicMock()
        self.job_service._add_test_fields(mock_test, include_all=True)
        
        # All field groups should be included
        mock_test.name.assert_called_once()
        mock_test.run_id.assert_called_once()
        mock_test.execution_time.assert_called_once()
        mock_test.status.assert_called_once()
        mock_test.raw_sql.assert_called_once()
>>>>>>> REPLACE
```

Now let's add tests for the methods that use these field selection options:

tests/test_job_service.py
```python
<<<<<<< SEARCH
    def test_get_job_models(self):
        """Test get_job_models method."""
        # Configure mock response
        self.mock_base_query.execute.return_value = self.mock_job_response
        
        # Call the method
        result = self.job_service.get_job_models(self.test_job_id)
        
        # Verify method calls
        self.mock_base_query.create_job_query.assert_called_once_with(self.test_job_id)
        self.mock_job.models.assert_called_once()
        self.mock_base_query.execute.assert_called_once_with(self.mock_op)
        
        # Verify result
        self.assertEqual(result["id"], self.test_job_id)
        self.assertEqual(len(result["models"]), 1)
        self.assertEqual(result["models"][0]["unique_id"], self.test_unique_id)
=======
    def test_get_job_models(self):
        """Test get_job_models method."""
        # Configure mock response
        self.mock_base_query.execute.return_value = self.mock_job_response
        
        # Call the method
        result = self.job_service.get_job_models(self.test_job_id)
        
        # Verify method calls
        self.mock_base_query.create_job_query.assert_called_once_with(self.test_job_id)
        self.mock_job.models.assert_called_once()
        self.mock_base_query.execute.assert_called_once_with(self.mock_op)
        
        # Verify result
        self.assertEqual(result["id"], self.test_job_id)
        self.assertEqual(len(result["models"]), 1)
        self.assertEqual(result["models"][0]["unique_id"], self.test_unique_id)
    
    def test_get_job_models_with_field_selection(self):
        """Test get_job_models method with field selection options."""
        # Configure mock response
        self.mock_base_query.execute.return_value = self.mock_job_response
        
        # Reset mock to clear previous calls
        self.mock_base_query.reset_mock()
        self.mock_job.reset_mock()
        
        # Call the method with field selection options
        result = self.job_service.get_job_models(
            self.test_job_id, 
            include_database=True, 
            include_code=True,
            return_query=True
        )
        
        # Verify method calls
        self.mock_base_query.create_job_query.assert_called_once_with(self.test_job_id)
        self.mock_job.models.assert_called_once()
        
        # Verify execute was called with return_query=True
        self.mock_base_query.execute.assert_called_once_with(self.mock_op, return_query=True)
>>>>>>> REPLACE
```

tests/test_job_service.py
```python
<<<<<<< SEARCH
    def test_get_job_tests(self):
        """Test get_job_tests method."""
        # Configure mock response
        self.mock_base_query.execute.return_value = self.mock_job_response
        
        # Call the method
        result = self.job_service.get_job_tests(self.test_job_id)
        
        # Verify method calls
        self.mock_base_query.create_job_query.assert_called_once_with(self.test_job_id)
        self.mock_job.tests.assert_called_once()
        self.mock_base_query.execute.assert_called_once_with(self.mock_op)
        
        # Verify result
        self.assertEqual(result["id"], self.test_job_id)
        self.assertEqual(len(result["tests"]), 1)
        self.assertEqual(result["tests"][0]["unique_id"], "test.test_project.test_assertion")
=======
    def test_get_job_tests(self):
        """Test get_job_tests method."""
        # Configure mock response
        self.mock_base_query.execute.return_value = self.mock_job_response
        
        # Call the method
        result = self.job_service.get_job_tests(self.test_job_id)
        
        # Verify method calls
        self.mock_base_query.create_job_query.assert_called_once_with(self.test_job_id)
        self.mock_job.tests.assert_called_once()
        self.mock_base_query.execute.assert_called_once_with(self.mock_op)
        
        # Verify result
        self.assertEqual(result["id"], self.test_job_id)
        self.assertEqual(len(result["tests"]), 1)
        self.assertEqual(result["tests"][0]["unique_id"], "test.test_project.test_assertion")
    
    def test_get_job_tests_with_field_selection(self):
        """Test get_job_tests method with field selection options."""
        # Configure mock response
        self.mock_base_query.execute.return_value = self.mock_job_response
        
        # Reset mock to clear previous calls
        self.mock_base_query.reset_mock()
        self.mock_job.reset_mock()
        
        # Call the method with field selection options
        result = self.job_service.get_job_tests(
            self.test_job_id, 
            include_status=True, 
            include_timing=True,
            return_query=True
        )
        
        # Verify method calls
        self.mock_base_query.create_job_query.assert_called_once_with(self.test_job_id)
        self.mock_job.tests.assert_called_once()
        
        # Verify execute was called with return_query=True
        self.mock_base_query.execute.assert_called_once_with(self.mock_op, return_query=True)
>>>>>>> REPLACE
```

tests/test_job_service.py
```python
<<<<<<< SEARCH
    def test_get_job_with_models_and_tests(self):
        """Test get_job_with_models_and_tests method."""
        # Configure mock response
        self.mock_base_query.execute.return_value = self.mock_job_response
        
        # Call the method
        result = self.job_service.get_job_with_models_and_tests(self.test_job_id)
        
        # Verify method calls
        self.mock_base_query.create_job_query.assert_called_once_with(self.test_job_id)
        self.mock_job.models.assert_called_once()
        self.mock_job.tests.assert_called_once()
        self.mock_base_query.execute.assert_called_once_with(self.mock_op)
        
        # Verify result
        self.assertEqual(result["id"], self.test_job_id)
        self.assertEqual(len(result["models"]), 1)
        self.assertEqual(result["models"][0]["unique_id"], self.test_unique_id)
        self.assertEqual(len(result["tests"]), 1)
        self.assertEqual(result["tests"][0]["unique_id"], "test.test_project.test_assertion")
=======
    def test_get_job_with_models_and_tests(self):
        """Test get_job_with_models_and_tests method."""
        # Configure mock response
        self.mock_base_query.execute.return_value = self.mock_job_response
        
        # Call the method
        result = self.job_service.get_job_with_models_and_tests(self.test_job_id)
        
        # Verify method calls
        self.mock_base_query.create_job_query.assert_called_once_with(self.test_job_id)
        self.mock_job.models.assert_called_once()
        self.mock_job.tests.assert_called_once()
        self.mock_base_query.execute.assert_called_once_with(self.mock_op)
        
        # Verify result
        self.assertEqual(result["id"], self.test_job_id)
        self.assertEqual(len(result["models"]), 1)
        self.assertEqual(result["models"][0]["unique_id"], self.test_unique_id)
        self.assertEqual(len(result["tests"]), 1)
        self.assertEqual(result["tests"][0]["unique_id"], "test.test_project.test_assertion")
    
    def test_get_job_with_models_and_tests_with_field_selection(self):
        """Test get_job_with_models_and_tests method with field selection options."""
        # Configure mock response
        self.mock_base_query.execute.return_value = self.mock_job_response
        
        # Reset mock to clear previous calls
        self.mock_base_query.reset_mock()
        self.mock_job.reset_mock()
        
        # Call the method with field selection options
        result = self.job_service.get_job_with_models_and_tests(
            self.test_job_id, 
            include_all=True,
            return_query=True
        )
        
        # Verify method calls
        self.mock_base_query.create_job_query.assert_called_once_with(self.test_job_id)
        self.mock_job.models.assert_called_once()
        self.mock_job.tests.assert_called_once()
        
        # Verify execute was called with return_query=True
        self.mock_base_query.execute.assert_called_once_with(self.mock_op, return_query=True)
>>>>>>> REPLACE
```

Let's also fix a bug in the get_job_models method where we're not properly passing the exec_kwargs:

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
        response = self.base_query.execute(op, **kwargs)
=======
        response = self.base_query.execute(op, **exec_kwargs)
>>>>>>> REPLACE
```

Now you can run the tests to verify the new functionality:

```bash
python -m pytest tests/test_job_service.py -v
```

> Tokens: 16k sent, 3.4k received. Cost: $0.10 message, $0.17 session.  
> Applied edit to src/discovery_api/services/JobService.py  
> Applied edit to tests/test_job_service.py  
>  
> # Fix any errors below, if possible.  
>  
> ## Running: /Users/doug/.local/share/uv/tools/aider-chat/bin/python3 -m flake8 --select=E9,F821,F823,F831,F406,F407,F701,F702,F704,F706 --show-source --isolated src/discovery_api/services/JobService.py  
>  
> src/discovery_api/services/JobService.py:173:50: F821 undefined name 'exec_kwargs'  
> response = self.base_query.execute(op, **exec_kwargs)  
> ^  
>  
> ## See relevant line below marked with .  
>  
> src/discovery_api/services/JobService.py:  
> ......  
> 7class JobService:  
> 8    """Service for querying job data."""  
> 9  
> ......  
> 159    def get_job_metadata(self, job_id: int, **kwargs) -> Dict[str, Any]:  
> 160        """  
> 161        Get job metadata.  
> 162  
> 163        Args:  
> 164            job_id: The ID of the job to query  
> 165            **kwargs: Additional arguments to pass to execute, such as return_query=True  
> 166  
> 167        Returns:  
> 168            Dictionary containing job metadata  
> 169        """  
> 170        op, job = self.base_query.create_job_query(job_id)  
> 171        self._add_job_fields(job)  
> 172  
> 173        response = self.base_query.execute(op, **exec_kwargs)  
> 174        result = response["data"]["job"]  
> 175  
> 176        return cast(Dict[str, Any], self._convert_keys_to_snake_case(result))  
> 177  
> ......  
>  
> Attempt to fix lint errors? (Y)es/(N)o [Yes]: y  

I'll fix the undefined `exec_kwargs` variable in the `get_job_metadata` method. The issue is that we need to extract the execution kwargs from the input kwargs, similar to how it's done in the other methods.

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def get_job_metadata(self, job_id: int, **kwargs) -> Dict[str, Any]:
        """
        Get job metadata.
        
        Args:
            job_id: The ID of the job to query
            **kwargs: Additional arguments to pass to execute, such as return_query=True
            
        Returns:
            Dictionary containing job metadata
        """
        op, job = self.base_query.create_job_query(job_id)
        self._add_job_fields(job)
        
        response = self.base_query.execute(op, **exec_kwargs)
=======
    def get_job_metadata(self, job_id: int, **kwargs) -> Dict[str, Any]:
        """
        Get job metadata.
        
        Args:
            job_id: The ID of the job to query
            **kwargs: Additional arguments to pass to execute, such as return_query=True
            
        Returns:
            Dictionary containing job metadata
        """
        # Extract execution kwargs
        exec_kwargs = {k: v for k, v in kwargs.items() if k in ['return_query']}
        
        op, job = self.base_query.create_job_query(job_id)
        self._add_job_fields(job)
        
        response = self.base_query.execute(op, **exec_kwargs)
>>>>>>> REPLACE
```

There's also another issue in the `get_job_models` method where we're using `kwargs` instead of `exec_kwargs`:

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
        response = self.base_query.execute(op, **kwargs)
=======
        response = self.base_query.execute(op, **exec_kwargs)
>>>>>>> REPLACE
```

Now you can run the flake8 check again to verify the fixes:

```bash
python -m flake8 --select=E9,F821,F823,F831,F406,F407,F701,F702,F704,F706 --show-source --isolated src/discovery_api/services/JobService.py
```

> Tokens: 21k sent, 451 received. Cost: $0.07 message, $0.24 session.  
> Applied edit to src/discovery_api/services/JobService.py  
> python -m pytest tests/test_job_service.py -v  
> Run shell command? (Y)es/(N)o/(S)kip all/(D)on't ask again [Yes]: n  
> python -m flake8 --select=E9,F821,F823,F831,F406,F407,F701,F702,F704,F706 --show-source --isolated src/discovery_api/services/JobService.py  
> Run shell command? (Y)es/(N)o/(S)kip all/(D)on't ask again [Yes]: n  

#### ================================================================================================================================================= FAILURES ==================================================================================================================================================  
#### ___________________________________________________________________________________________________________________________________ TestJobService.test_add_model_fields ____________________________________________________________________________________________________________________________________  
####   
#### self = <tests.test_job_service.TestJobService testMethod=test_add_model_fields>  
####   
####     def test_add_model_fields(self):  
####         """Test _add_model_fields method with default settings (all fields)."""  
####         mock_model = MagicMock()  
####         self.job_service._add_model_fields(mock_model)  
####       
####         # Verify basic fields are added  
####         mock_model.name.assert_called_once()  
####         mock_model.unique_id.assert_called_once()  
####         mock_model.description.assert_called_once()  
####         mock_model.tags.assert_called_once()  
####         mock_model.resource_type.assert_called_once()  
####       
####         # Verify database fields are added  
#### >       mock_model.database.assert_called_once()  
####   
#### tests/test_job_service.py:141:   
#### _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
####   
#### self = <MagicMock name='mock.database' id='4360533920'>  
####   
####     def assert_called_once(self):  
####         """assert that the mock was called only once.  
####         """  
####         if not self.call_count == 1:  
####             msg = ("Expected '%s' to have been called once. Called %s times.%s"  
####                    % (self._mock_name or 'mock',  
####                       self.call_count,  
####                       self._calls_repr()))  
#### >           raise AssertionError(msg)  
#### E           AssertionError: Expected 'database' to have been called once. Called 0 times.  
####   
#### /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:923: AssertionError  
#### ============================================================================================================================================= warnings summary ==============================================================================================================================================  
#### .venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:192  
####   /Users/doug/Documents/GitHub/dbt_discovery_api/.venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name "schema" in "ModelMetadata" shadows an attribute in parent "BaseModel"  
####     warnings.warn(  
####   
#### -- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html  
#### ========================================================================================================================================== short test summary info ==========================================================================================================================================  
#### FAILED tests/test_job_service.py::TestJobService::test_add_model_fields - AssertionError: Expected 'database' to have been called once. Called 0 times.  
#### !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !  
> https://docs.pytest.org/en/stable/how-to/capture-warnings.html  
> Add URL to the chat? (Y)es/(N)o/(D)on't ask again [Yes]: y  
> Scraping https://docs.pytest.org/en/stable/how-to/capture-warnings.html...  

I'll fix the test failures by updating the JobService implementation. The issue is that the default behavior of `_add_model_fields` and `_add_test_fields` has changed - they now require explicit flags to include fields beyond the basic ones, but the tests expect all fields to be included by default.

Here are the changes needed:

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def _add_model_fields(self, models: Operation, **kwargs):
        """
        Add model fields to the query.
        
        Args:
            models: The Operation object to add fields to
            **kwargs: Optional field groups to include:
                include_database (bool): Include database fields
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
        """
        # Default field groups to include
        include_all = kwargs.get('include_all', False)
        include_database = kwargs.get('include_database', include_all)
        include_run_metadata = kwargs.get('include_run_metadata', include_all)
        include_timing = kwargs.get('include_timing', include_all)
        include_status = kwargs.get('include_status', include_all)
        include_code = kwargs.get('include_code', include_all)
=======
    def _add_model_fields(self, models: Operation, **kwargs):
        """
        Add model fields to the query.
        
        Args:
            models: The Operation object to add fields to
            **kwargs: Optional field groups to include:
                include_database (bool): Include database fields
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
        """
        # Default field groups to include - by default include all fields for backward compatibility
        include_all = kwargs.get('include_all', True)
        include_database = kwargs.get('include_database', include_all)
        include_run_metadata = kwargs.get('include_run_metadata', include_all)
        include_timing = kwargs.get('include_timing', include_all)
        include_status = kwargs.get('include_status', include_all)
        include_code = kwargs.get('include_code', include_all)
>>>>>>> REPLACE
```

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def _add_test_fields(self, tests: Operation, **kwargs):
        """
        Add test fields to the query.
        
        Args:
            tests: The Operation object to add fields to
            **kwargs: Optional field groups to include:
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
        """
        # Default field groups to include
        include_all = kwargs.get('include_all', False)
        include_run_metadata = kwargs.get('include_run_metadata', include_all)
        include_timing = kwargs.get('include_timing', include_all)
        include_status = kwargs.get('include_status', include_all)
        include_code = kwargs.get('include_code', include_all)
=======
    def _add_test_fields(self, tests: Operation, **kwargs):
        """
        Add test fields to the query.
        
        Args:
            tests: The Operation object to add fields to
            **kwargs: Optional field groups to include:
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
        """
        # Default field groups to include - by default include all fields for backward compatibility
        include_all = kwargs.get('include_all', True)
        include_run_metadata = kwargs.get('include_run_metadata', include_all)
        include_timing = kwargs.get('include_timing', include_all)
        include_status = kwargs.get('include_status', include_all)
        include_code = kwargs.get('include_code', include_all)
>>>>>>> REPLACE
```

Now you can run the tests again to verify the fix:

```bash
python -m pytest tests/test_job_service.py -v
```

> Tokens: 28k sent, 922 received. Cost: $0.10 message, $0.34 session.  
> Applied edit to src/discovery_api/services/JobService.py  
> python -m pytest tests/test_job_service.py -v  
> Run shell command? (Y)es/(N)o/(D)on't ask again [Yes]: n  

#### /clear  

####   
#### ================================================================================================================================================= FAILURES ==================================================================================================================================================  
#### ________________________________________________________________________________________________________________________ TestJobService.test_add_model_fields_with_selective_fields _________________________________________________________________________________________________________________________  
####   
#### self = <tests.test_job_service.TestJobService testMethod=test_add_model_fields_with_selective_fields>  
####   
####     def test_add_model_fields_with_selective_fields(self):  
####         """Test _add_model_fields method with selective field groups."""  
####         # Test with only database fields  
####         mock_model = MagicMock()  
####         self.job_service._add_model_fields(mock_model, include_database=True)  
####       
####         # Basic fields should always be included  
####         mock_model.name.assert_called_once()  
####         mock_model.unique_id.assert_called_once()  
####         mock_model.resource_type.assert_called_once()  
####       
####         # Database fields should be included  
####         mock_model.database.assert_called_once()  
####         mock_model.schema.assert_called_once()  
####       
####         # Run metadata fields should not be included  
#### >       mock_model.run_id.assert_not_called()  
####   
#### tests/test_job_service.py:177:   
#### _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
####   
#### self = <MagicMock name='mock.run_id' id='4460064992'>  
####   
####     def assert_not_called(self):  
####         """assert that the mock was never called.  
####         """  
####         if self.call_count != 0:  
####             msg = ("Expected '%s' to not have been called. Called %s times.%s"  
####                    % (self._mock_name or 'mock',  
####                       self.call_count,  
####                       self._calls_repr()))  
#### >           raise AssertionError(msg)  
#### E           AssertionError: Expected 'run_id' to not have been called. Called 1 times.  
#### E           Calls: [call()].  
####   
#### /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:905: AssertionError  

I see the issue. The test is failing because the `_add_model_fields` method in `JobService` is still calling `run_id()` even when `include_database=True` but `include_run_metadata=False`. Let's fix this by modifying the `_add_model_fields` method to properly respect the field selection parameters.

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def _add_model_fields(self, models: Operation, **kwargs):
        """
        Add model fields to the query.
        
        Args:
            models: The Operation object to add fields to
            **kwargs: Optional field groups to include:
                include_database (bool): Include database fields
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
        """
        # Default field groups to include - by default include all fields for backward compatibility
        include_all = kwargs.get('include_all', True)
        include_database = kwargs.get('include_database', include_all)
        include_run_metadata = kwargs.get('include_run_metadata', include_all)
        include_timing = kwargs.get('include_timing', include_all)
        include_status = kwargs.get('include_status', include_all)
        include_code = kwargs.get('include_code', include_all)
=======
    def _add_model_fields(self, models: Operation, **kwargs):
        """
        Add model fields to the query.
        
        Args:
            models: The Operation object to add fields to
            **kwargs: Optional field groups to include:
                include_database (bool): Include database fields
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
        """
        # Check if specific field groups are explicitly set
        has_explicit_field_groups = any(
            key in kwargs for key in [
                'include_database', 'include_run_metadata', 
                'include_timing', 'include_status', 'include_code'
            ]
        )
        
        # Default field groups to include
        # If any specific field group is set, don't use include_all as default
        default_include_all = not has_explicit_field_groups
        
        include_all = kwargs.get('include_all', default_include_all)
        include_database = kwargs.get('include_database', include_all)
        include_run_metadata = kwargs.get('include_run_metadata', include_all)
        include_timing = kwargs.get('include_timing', include_all)
        include_status = kwargs.get('include_status', include_all)
        include_code = kwargs.get('include_code', include_all)
>>>>>>> REPLACE
```

Let's also update the `_add_test_fields` method to have the same behavior for consistency:

src/discovery_api/services/JobService.py
```python
<<<<<<< SEARCH
    def _add_test_fields(self, tests: Operation, **kwargs):
        """
        Add test fields to the query.
        
        Args:
            tests: The Operation object to add fields to
            **kwargs: Optional field groups to include:
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
        """
        # Default field groups to include - by default include all fields for backward compatibility
        include_all = kwargs.get('include_all', True)
        include_run_metadata = kwargs.get('include_run_metadata', include_all)
        include_timing = kwargs.get('include_timing', include_all)
        include_status = kwargs.get('include_status', include_all)
        include_code = kwargs.get('include_code', include_all)
=======
    def _add_test_fields(self, tests: Operation, **kwargs):
        """
        Add test fields to the query.
        
        Args:
            tests: The Operation object to add fields to
            **kwargs: Optional field groups to include:
                include_run_metadata (bool): Include run metadata fields
                include_timing (bool): Include execution timing fields
                include_status (bool): Include status information fields
                include_code (bool): Include code fields
                include_all (bool): Include all fields
        """
        # Check if specific field groups are explicitly set
        has_explicit_field_groups = any(
            key in kwargs for key in [
                'include_run_metadata', 'include_timing', 
                'include_status', 'include_code'
            ]
        )
        
        # Default field groups to include
        # If any specific field group is set, don't use include_all as default
        default_include_all = not has_explicit_field_groups
        
        include_all = kwargs.get('include_all', default_include_all)
        include_run_metadata = kwargs.get('include_run_metadata', include_all)
        include_timing = kwargs.get('include_timing', include_all)
        include_status = kwargs.get('include_status', include_all)
        include_code = kwargs.get('include_code', include_all)
>>>>>>> REPLACE
```

Now let's run the tests to see if our changes fixed the issue:

```bash
python -m pytest tests/test_job_service.py -v
```

> Tokens: 14k sent, 1.1k received. Cost: $0.06 message, $0.40 session.  
> Applied edit to src/discovery_api/services/JobService.py  
> python -m pytest tests/test_job_service.py -v  
> Run shell command? (Y)es/(N)o/(D)on't ask again [Yes]: n  

#### /commit  
