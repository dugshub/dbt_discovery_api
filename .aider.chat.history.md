
# aider chat started at 2025-03-06 00:14:06

> You can skip this check with --no-gitignore  
> Add .aider*, .env to .gitignore (recommended)? (Y)es/(N)o [Yes]: y  
> Added .aider*, .env to .gitignore  
> /Users/dug/.local/bin/aider --model claude-3-7-sonnet-20250219 --yes-always --no-auto-commit --map-tokens 2048  
> Aider v0.75.2  
> Main model: claude-3-7-sonnet-20250219 with diff edit format, infinite output  
> Weak model: claude-3-5-haiku-20241022  
> Git repo: .git with 45 files  
> Repo-map: using 2048 tokens, auto refresh  

#### /add sql_gen.py  
> No files matched 'sql_gen.py'. Do you want to create /Users/dug/projects/Official Projects/dbt-tools/dbt_discovery_api/sql_gen.py? (Y)es/(N)o [Yes]: y  
> Added sql_gen.py to the chat  

#### /read-only sql_gen.py  
> Moved /Users/dug/projects/Official Projects/dbt-tools/dbt_discovery_api/sql_gen.py from editable to read-only files in the chat  

#### /add redshift_agent.py  
> No files matched 'redshift_agent.py'. Do you want to create /Users/dug/projects/Official Projects/dbt-tools/dbt_discovery_api/redshift_agent.py? (Y)es/(N)o [Yes]: y  
> Added redshift_agent.py to the chat  

#### CREATE redshift_agent.py MIRROR sql_gen.py - convert where needed to use Redshift functions. Ensure the agent knows its using redshift.  

I'll create a new `redshift_agent.py` file based on `sql_gen.py`, adapting it to use Redshift-specific functions and ensuring the agent knows it's working with Redshift.

redshift_agent.py
```python
<<<<<<< SEARCH
=======
import os
import re
import json
import logging
from typing import List, Dict, Any, Optional, Union, Tuple

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools.base import BaseTool
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RedshiftSQLGenerator:
    """
    A class that uses LLM to generate and refine Redshift SQL queries.
    """
    
    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.0):
        """
        Initialize the RedshiftSQLGenerator with a specific model.
        
        Args:
            model_name: The OpenAI model to use
            temperature: The temperature setting for the model
        """
        self.model_name = model_name
        self.temperature = temperature
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        
        # Create tools for the agent
        self.tools = self._create_tools()
        
        # Create the agent with the tools
        self.agent = self._create_agent()
        
        # Create the agent executor
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True,
        )
        
        # Initialize chat history
        self.chat_history = []
    
    def _create_tools(self) -> List[BaseTool]:
        """
        Create tools for the agent to use.
        
        Returns:
            A list of tools
        """
        # For now, we don't have any specific tools
        # This could be extended with database schema tools, etc.
        return []
    
    def _create_agent(self):
        """
        Create an agent with the tools.
        
        Returns:
            An agent
        """
        system_message = """You are an expert Redshift SQL developer. 
Your job is to write efficient, correct SQL queries for Redshift databases.

When writing SQL:
- Use Redshift-specific functions and syntax where appropriate
- Consider performance implications of your queries
- Use appropriate Redshift data types
- Follow Redshift best practices for query optimization
- Use DISTKEY and SORTKEY appropriately when creating tables
- Consider using Redshift's columnar storage optimizations
- Leverage Redshift's parallel processing capabilities
- Use appropriate compression encodings for columns

Redshift-specific functions to consider:
- COPY for data loading
- UNLOAD for data extraction
- VACUUM for table maintenance
- ANALYZE for statistics updates
- Window functions with OVER clause
- System tables like SVV_TABLE_INFO, STL_QUERY for monitoring
- Redshift Spectrum for querying data in S3

Always explain your reasoning and any Redshift-specific optimizations you've included.
"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        return create_openai_tools_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )
    
    def generate_sql(self, query_description: str) -> str:
        """
        Generate a SQL query based on a description.
        
        Args:
            query_description: A description of the query to generate
            
        Returns:
            A SQL query
        """
        # Add context about Redshift to the query description
        enhanced_description = f"""
Generate a Redshift SQL query that: {query_description}

Remember to use Redshift-specific features and optimizations where appropriate.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": enhanced_description,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=enhanced_description))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return sql
    
    def refine_sql(self, sql: str, refinement_instructions: str) -> str:
        """
        Refine an existing SQL query based on instructions.
        
        Args:
            sql: The SQL query to refine
            refinement_instructions: Instructions for how to refine the query
            
        Returns:
            A refined SQL query
        """
        # Add context about Redshift to the refinement instructions
        enhanced_instructions = f"""
Refine this Redshift SQL query:

```sql
{sql}
```

Refinement needed: {refinement_instructions}

Remember to use Redshift-specific features and optimizations where appropriate.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": enhanced_instructions,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        refined_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=enhanced_instructions))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return refined_sql
    
    def explain_sql(self, sql: str) -> str:
        """
        Explain a SQL query in natural language.
        
        Args:
            sql: The SQL query to explain
            
        Returns:
            A natural language explanation of the query
        """
        prompt = f"""
Explain this Redshift SQL query in detail:

```sql
{sql}
```

Include explanations of:
1. What the query does
2. Any Redshift-specific functions or optimizations
3. How the query is structured
4. Any performance considerations
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def optimize_sql(self, sql: str) -> str:
        """
        Optimize a SQL query for better performance.
        
        Args:
            sql: The SQL query to optimize
            
        Returns:
            An optimized SQL query
        """
        prompt = f"""
Optimize this Redshift SQL query for better performance:

```sql
{sql}
```

Consider Redshift-specific optimizations such as:
1. Appropriate DISTKEY and SORTKEY usage
2. Efficient join strategies
3. Avoiding expensive operations
4. Using appropriate compression encodings
5. Leveraging Redshift's parallel processing
6. Minimizing data movement between slices
7. Using appropriate Redshift functions

Explain your optimizations.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        optimized_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return optimized_sql
    
    def _extract_sql(self, text: str) -> str:
        """
        Extract SQL code from a text that may contain markdown code blocks.
        
        Args:
            text: The text to extract SQL from
            
        Returns:
            The extracted SQL
        """
        # Try to extract SQL from markdown code blocks
        sql_blocks = re.findall(r"```sql\n(.*?)\n```", text, re.DOTALL)
        
        if sql_blocks:
            return sql_blocks[0].strip()
        
        # If no SQL code blocks, try generic code blocks
        code_blocks = re.findall(r"```\n(.*?)\n```", text, re.DOTALL)
        
        if code_blocks:
            return code_blocks[0].strip()
        
        # If no code blocks at all, return the original text
        # This is a fallback and might not be actual SQL
        return text.strip()
    
    def clear_history(self):
        """
        Clear the chat history.
        """
        self.chat_history = []
    
    def generate_table_creation(self, table_description: str) -> str:
        """
        Generate a CREATE TABLE statement based on a description.
        
        Args:
            table_description: A description of the table to create
            
        Returns:
            A CREATE TABLE statement
        """
        prompt = f"""
Generate a Redshift CREATE TABLE statement based on this description:

{table_description}

Include appropriate:
1. Column data types optimized for Redshift
2. DISTKEY and SORTKEY definitions
3. Compression encodings where beneficial
4. Primary keys and foreign keys if needed
5. Any other Redshift-specific optimizations

Explain your choices for distribution style, sort keys, and compression.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        create_table_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return create_table_sql
    
    def generate_data_loading(self, table_name: str, data_source: str) -> str:
        """
        Generate a COPY command for loading data into Redshift.
        
        Args:
            table_name: The name of the table to load data into
            data_source: Description of the data source (S3 path, format, etc.)
            
        Returns:
            A COPY command
        """
        prompt = f"""
Generate a Redshift COPY command to load data into the table '{table_name}' from this source:

{data_source}

Include appropriate:
1. Data format specifications
2. Error handling options
3. IAM role or credentials method
4. Compression options if applicable
5. Any other relevant COPY command parameters

Explain your choices for the COPY command parameters.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        copy_command = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return copy_command
>>>>>>> REPLACE
```

I've created a new `redshift_agent.py` file that mirrors the structure of `sql_gen.py` but is specifically tailored for Redshift. The key changes include:

1. Renamed the main class to `RedshiftSQLGenerator`
2. Updated the system message to include Redshift-specific knowledge
3. Added Redshift-specific functions and optimizations in the prompts
4. Added methods for Redshift-specific operations like table creation with DISTKEY/SORTKEY and data loading with COPY commands
5. Included references to Redshift-specific features like compression encodings, distribution styles, and system tables

You can now use this agent to generate Redshift-optimized SQL queries.

> Tokens: 4.4k sent, 2.4k received. Cost: $0.05 message, $0.05 session.  
> Applied edit to redshift_agent.py  

#### remove all create functionality. Exclusively allow read. The main purpose of this agent is to work alongside the README.md dbt API to help identify redshift performane issues. This one specifically will need to know which redshift tables to look at  
> README.md  
> Add file to the chat? (Y)es/(N)o/(D)on't ask again [Yes]: y  

I'll modify the `redshift_agent.py` file to focus exclusively on read functionality for identifying Redshift performance issues, removing all create functionality, and ensuring it works alongside the dbt API described in the README.md.

redshift_agent.py
````python
<<<<<<< SEARCH
import os
import re
import json
import logging
from typing import List, Dict, Any, Optional, Union, Tuple

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools.base import BaseTool
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RedshiftSQLGenerator:
    """
    A class that uses LLM to generate and refine Redshift SQL queries.
    """
    
    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.0):
        """
        Initialize the RedshiftSQLGenerator with a specific model.
        
        Args:
            model_name: The OpenAI model to use
            temperature: The temperature setting for the model
        """
        self.model_name = model_name
        self.temperature = temperature
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        
        # Create tools for the agent
        self.tools = self._create_tools()
        
        # Create the agent with the tools
        self.agent = self._create_agent()
        
        # Create the agent executor
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True,
        )
        
        # Initialize chat history
        self.chat_history = []
    
    def _create_tools(self) -> List[BaseTool]:
        """
        Create tools for the agent to use.
        
        Returns:
            A list of tools
        """
        # For now, we don't have any specific tools
        # This could be extended with database schema tools, etc.
        return []
    
    def _create_agent(self):
        """
        Create an agent with the tools.
        
        Returns:
            An agent
        """
        system_message = """You are an expert Redshift SQL developer. 
Your job is to write efficient, correct SQL queries for Redshift databases.

When writing SQL:
- Use Redshift-specific functions and syntax where appropriate
- Consider performance implications of your queries
- Use appropriate Redshift data types
- Follow Redshift best practices for query optimization
- Use DISTKEY and SORTKEY appropriately when creating tables
- Consider using Redshift's columnar storage optimizations
- Leverage Redshift's parallel processing capabilities
- Use appropriate compression encodings for columns

Redshift-specific functions to consider:
- COPY for data loading
- UNLOAD for data extraction
- VACUUM for table maintenance
- ANALYZE for statistics updates
- Window functions with OVER clause
- System tables like SVV_TABLE_INFO, STL_QUERY for monitoring
- Redshift Spectrum for querying data in S3

Always explain your reasoning and any Redshift-specific optimizations you've included.
"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        return create_openai_tools_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )
    
    def generate_sql(self, query_description: str) -> str:
        """
        Generate a SQL query based on a description.
        
        Args:
            query_description: A description of the query to generate
            
        Returns:
            A SQL query
        """
        # Add context about Redshift to the query description
        enhanced_description = f"""
Generate a Redshift SQL query that: {query_description}

Remember to use Redshift-specific features and optimizations where appropriate.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": enhanced_description,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=enhanced_description))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return sql
    
    def refine_sql(self, sql: str, refinement_instructions: str) -> str:
        """
        Refine an existing SQL query based on instructions.
        
        Args:
            sql: The SQL query to refine
            refinement_instructions: Instructions for how to refine the query
            
        Returns:
            A refined SQL query
        """
        # Add context about Redshift to the refinement instructions
        enhanced_instructions = f"""
Refine this Redshift SQL query:

```sql
{sql}
```

Refinement needed: {refinement_instructions}

Remember to use Redshift-specific features and optimizations where appropriate.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": enhanced_instructions,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        refined_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=enhanced_instructions))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return refined_sql
    
    def explain_sql(self, sql: str) -> str:
        """
        Explain a SQL query in natural language.
        
        Args:
            sql: The SQL query to explain
            
        Returns:
            A natural language explanation of the query
        """
        prompt = f"""
Explain this Redshift SQL query in detail:

```sql
{sql}
```

Include explanations of:
1. What the query does
2. Any Redshift-specific functions or optimizations
3. How the query is structured
4. Any performance considerations
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def optimize_sql(self, sql: str) -> str:
        """
        Optimize a SQL query for better performance.
        
        Args:
            sql: The SQL query to optimize
            
        Returns:
            An optimized SQL query
        """
        prompt = f"""
Optimize this Redshift SQL query for better performance:

```sql
{sql}
```

Consider Redshift-specific optimizations such as:
1. Appropriate DISTKEY and SORTKEY usage
2. Efficient join strategies
3. Avoiding expensive operations
4. Using appropriate compression encodings
5. Leveraging Redshift's parallel processing
6. Minimizing data movement between slices
7. Using appropriate Redshift functions

Explain your optimizations.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        optimized_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return optimized_sql
    
    def _extract_sql(self, text: str) -> str:
        """
        Extract SQL code from a text that may contain markdown code blocks.
        
        Args:
            text: The text to extract SQL from
            
        Returns:
            The extracted SQL
        """
        # Try to extract SQL from markdown code blocks
        sql_blocks = re.findall(r"```sql\n(.*?)\n```", text, re.DOTALL)
        
        if sql_blocks:
            return sql_blocks[0].strip()
        
        # If no SQL code blocks, try generic code blocks
        code_blocks = re.findall(r"```\n(.*?)\n```", text, re.DOTALL)
        
        if code_blocks:
            return code_blocks[0].strip()
        
        # If no code blocks at all, return the original text
        # This is a fallback and might not be actual SQL
        return text.strip()
    
    def clear_history(self):
        """
        Clear the chat history.
        """
        self.chat_history = []
    
    def generate_table_creation(self, table_description: str) -> str:
        """
        Generate a CREATE TABLE statement based on a description.
        
        Args:
            table_description: A description of the table to create
            
        Returns:
            A CREATE TABLE statement
        """
        prompt = f"""
Generate a Redshift CREATE TABLE statement based on this description:

{table_description}

Include appropriate:
1. Column data types optimized for Redshift
2. DISTKEY and SORTKEY definitions
3. Compression encodings where beneficial
4. Primary keys and foreign keys if needed
5. Any other Redshift-specific optimizations

Explain your choices for distribution style, sort keys, and compression.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        create_table_sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return create_table_sql
    
    def generate_data_loading(self, table_name: str, data_source: str) -> str:
        """
        Generate a COPY command for loading data into Redshift.
        
        Args:
            table_name: The name of the table to load data into
            data_source: Description of the data source (S3 path, format, etc.)
            
        Returns:
            A COPY command
        """
        prompt = f"""
Generate a Redshift COPY command to load data into the table '{table_name}' from this source:

{data_source}

Include appropriate:
1. Data format specifications
2. Error handling options
3. IAM role or credentials method
4. Compression options if applicable
5. Any other relevant COPY command parameters

Explain your choices for the COPY command parameters.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        copy_command = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return copy_command
=======
import os
import re
import json
import logging
from typing import List, Dict, Any, Optional, Union, Tuple

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.tools.base import BaseTool
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RedshiftPerformanceAnalyzer:
    """
    A class that uses LLM to analyze and diagnose Redshift performance issues.
    Works alongside the dbt Discovery API to identify performance bottlenecks.
    """
    
    def __init__(self, model_name: str = "gpt-4o", temperature: float = 0.0):
        """
        Initialize the RedshiftPerformanceAnalyzer with a specific model.
        
        Args:
            model_name: The OpenAI model to use
            temperature: The temperature setting for the model
        """
        self.model_name = model_name
        self.temperature = temperature
        self.llm = ChatOpenAI(model=model_name, temperature=temperature)
        
        # Create tools for the agent
        self.tools = self._create_tools()
        
        # Create the agent with the tools
        self.agent = self._create_agent()
        
        # Create the agent executor
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True,
        )
        
        # Initialize chat history
        self.chat_history = []
    
    def _create_tools(self) -> List[BaseTool]:
        """
        Create tools for the agent to use.
        
        Returns:
            A list of tools
        """
        # For now, we don't have any specific tools
        # This could be extended with database schema tools, etc.
        return []
    
    def _create_agent(self):
        """
        Create an agent with the tools.
        
        Returns:
            An agent
        """
        system_message = """You are an expert Redshift performance analyst. 
Your job is to analyze and diagnose performance issues in Redshift databases.

When analyzing Redshift performance:
- Focus on identifying bottlenecks in query execution
- Recommend appropriate system tables to query for diagnostics
- Interpret query plans and execution statistics
- Identify distribution style and sort key issues
- Detect data skew problems
- Recognize inefficient join patterns
- Identify missing or outdated statistics

Key Redshift system tables to query:
- SVV_TABLE_INFO: Table metadata including distribution style, sort keys
- STL_QUERY: Query execution history
- STL_QUERY_METRICS: Detailed metrics about query steps
- STL_ALERT_EVENT_LOG: System-generated alerts
- SVL_QUERY_SUMMARY: Summary information for queries
- SVL_QUERY_REPORT: Step-level execution details
- SVV_QUERY_INFLIGHT: Currently running queries
- STV_INFLIGHT: Information about currently running queries
- STV_BLOCKLIST: Data block distribution
- STV_NODE_STORAGE_CAPACITY: Storage capacity and utilization

Always explain your reasoning and provide specific SQL queries to diagnose issues.
"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_message),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        return create_openai_tools_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )
    
    def generate_diagnostic_query(self, issue_description: str) -> str:
        """
        Generate a diagnostic SQL query based on a performance issue description.
        
        Args:
            issue_description: A description of the performance issue to diagnose
            
        Returns:
            A diagnostic SQL query
        """
        enhanced_description = f"""
Generate a Redshift diagnostic SQL query to investigate this performance issue: {issue_description}

Focus on using appropriate Redshift system tables and views to diagnose the problem.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": enhanced_description,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=enhanced_description))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return sql
    
    def analyze_query_performance(self, query_id: str) -> str:
        """
        Analyze the performance of a specific query by its ID.
        
        Args:
            query_id: The ID of the query to analyze
            
        Returns:
            Analysis of the query performance
        """
        prompt = f"""
Analyze the performance of Redshift query with ID {query_id}.

Generate SQL queries that use Redshift system tables to:
1. Get detailed execution metrics for this query
2. Identify bottlenecks in the query execution
3. Suggest potential optimizations

Include specific system tables like STL_QUERY, STL_QUERY_METRICS, SVL_QUERY_SUMMARY, etc.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def analyze_table_structure(self, table_name: str) -> str:
        """
        Analyze the structure of a specific table for performance issues.
        
        Args:
            table_name: The name of the table to analyze
            
        Returns:
            Analysis of the table structure
        """
        prompt = f"""
Analyze the structure of Redshift table '{table_name}' for potential performance issues.

Generate SQL queries that use Redshift system tables to:
1. Check the distribution style and sort keys
2. Analyze data skew
3. Check compression encodings
4. Verify statistics are up to date
5. Identify potential distribution or sort key improvements

Include specific system tables like SVV_TABLE_INFO, STV_BLOCKLIST, etc.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def identify_slow_queries(self, time_period: str = "1 day") -> str:
        """
        Identify slow queries within a specific time period.
        
        Args:
            time_period: The time period to analyze (e.g., "1 day", "1 week")
            
        Returns:
            SQL query to identify slow queries
        """
        prompt = f"""
Generate a Redshift SQL query to identify slow queries from the past {time_period}.

The query should:
1. Use appropriate system tables like STL_QUERY, SVL_QUERY_SUMMARY
2. Sort by execution time or other relevant performance metrics
3. Include query text, execution time, and other useful diagnostic information
4. Filter out system queries and focus on user queries
5. Provide context about what might be causing slowness
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Extract the SQL from the response
        sql = self._extract_sql(response["output"])
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return sql
    
    def analyze_workload_patterns(self) -> str:
        """
        Analyze workload patterns to identify peak usage and resource contention.
        
        Returns:
            Analysis of workload patterns
        """
        prompt = """
Generate Redshift SQL queries to analyze workload patterns and identify resource contention.

The queries should:
1. Identify peak usage times
2. Detect query queuing and WLM queue wait times
3. Analyze concurrency issues
4. Identify long-running queries that might block others
5. Check for appropriate WLM configuration

Use system tables like STL_QUERY, STL_WLM_QUERY, STL_COMMIT_STATS, etc.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def analyze_dbt_model_performance(self, model_name: str) -> str:
        """
        Analyze the performance of a specific dbt model in Redshift.
        
        Args:
            model_name: The name of the dbt model to analyze
            
        Returns:
            Analysis of the model's performance in Redshift
        """
        prompt = f"""
Generate Redshift SQL queries to analyze the performance of dbt model '{model_name}'.

Assume this model is materialized as a table or view in Redshift.

The queries should:
1. Check the table structure and distribution style
2. Analyze query patterns that access this model
3. Identify potential bottlenecks when this model is used in joins
4. Check for appropriate sort keys and distribution keys
5. Analyze data skew if present

Use system tables like SVV_TABLE_INFO, STL_QUERY, SVL_QUERY_SUMMARY, etc.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def _extract_sql(self, text: str) -> str:
        """
        Extract SQL code from a text that may contain markdown code blocks.
        
        Args:
            text: The text to extract SQL from
            
        Returns:
            The extracted SQL
        """
        # Try to extract SQL from markdown code blocks
        sql_blocks = re.findall(r"```sql\n(.*?)\n```", text, re.DOTALL)
        
        if sql_blocks:
            return sql_blocks[0].strip()
        
        # If no SQL code blocks, try generic code blocks
        code_blocks = re.findall(r"```\n(.*?)\n```", text, re.DOTALL)
        
        if code_blocks:
            return code_blocks[0].strip()
        
        # If no code blocks at all, return the original text
        # This is a fallback and might not be actual SQL
        return text.strip()
    
    def clear_history(self):
        """
        Clear the chat history.
        """
        self.chat_history = []
    
    def explain_system_table(self, table_name: str) -> str:
        """
        Explain a Redshift system table and how to use it for performance analysis.
        
        Args:
            table_name: The name of the system table to explain
            
        Returns:
            Explanation of the system table
        """
        prompt = f"""
Explain the Redshift system table '{table_name}' in detail:

1. What information does this table contain?
2. How is it useful for performance analysis?
3. What are the key columns to focus on?
4. Provide example queries that use this table for performance diagnostics
5. Mention any limitations or considerations when using this table
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
    
    def recommend_monitoring_queries(self) -> str:
        """
        Recommend a set of monitoring queries for ongoing Redshift performance tracking.
        
        Returns:
            A set of recommended monitoring queries
        """
        prompt = """
Recommend a set of Redshift SQL queries for ongoing performance monitoring.

Include queries that:
1. Track long-running queries
2. Monitor table usage and access patterns
3. Identify data skew issues
4. Track query queue wait times
5. Monitor disk usage and storage
6. Identify tables that need VACUUM or ANALYZE
7. Track query performance trends over time

Explain why each query is useful and how to interpret the results.
"""
        
        # Run the agent
        response = self.agent_executor.invoke({
            "input": prompt,
            "chat_history": self.chat_history
        })
        
        # Update chat history
        self.chat_history.append(HumanMessage(content=prompt))
        self.chat_history.append(AIMessage(content=response["output"]))
        
        return response["output"]
>>>>>>> REPLACE
````

I've completely refactored the file to:

1. Rename the class from `RedshiftSQLGenerator` to `RedshiftPerformanceAnalyzer` to better reflect its purpose
2. Removed all create functionality (table creation, data loading)
3. Updated the system message to focus on performance analysis rather than SQL generation
4. Added methods specifically for performance diagnostics:
   - `generate_diagnostic_query`: Creates queries to diagnose specific issues
   - `analyze_query_performance`: Analyzes performance of a specific query
   - `analyze_table_structure`: Examines table structure for performance issues
   - `identify_slow_queries`: Finds slow-running queries
   - `analyze_workload_patterns`: Examines resource usage patterns
   - `analyze_dbt_model_performance`: Specifically analyzes dbt models
   - `explain_system_table`: Explains Redshift system tables
   - `recommend_monitoring_queries`: Suggests monitoring queries

The agent now focuses exclusively on reading from Redshift system tables to diagnose performance issues, with specific knowledge about which tables to query for different types of problems. It's designed to work alongside the dbt Discovery API described in the README.md.

> Tokens: 11k sent, 5.3k received. Cost: $0.11 message, $0.16 session.  
> Applied edit to redshift_agent.py  
